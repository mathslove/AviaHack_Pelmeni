{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AviaHack (UTAir, AviaJob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Pelmeni** team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching between single CV & job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a533e43476c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use the application default credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCertificate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hrapp-b56d4-firebase-adminsdk-w16yw-8319963b70.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfirebase_admin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirestore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf_env/lib/python3.8/site-packages/firebase_admin/__init__.py\u001b[0m in \u001b[0;36minitialize_app\u001b[0;34m(credential, options, name)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DEFAULT_APP_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         raise ValueError((\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;34m'The default Firebase app already exists. This means you called '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;34m'initialize_app() more than once without providing an app name as '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "# Initializing FireBase admin\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import json\n",
    "\n",
    "# Use the application default credentials\n",
    "cred = credentials.Certificate(\"hrapp-b56d4-firebase-adminsdk-w16yw-8319963b70.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ИП Аэрофлотов Икар Петрович | Кадрова Екатерина Подборовна',\n",
       "  'title': 'Пилот воздушного судна',\n",
       "  'city': 'Москва',\n",
       "  'occupation': '5',\n",
       "  'weekend': '2',\n",
       "  'salary': '30000',\n",
       "  'responsibilities': ['Управление воздушным судном', 'Пилотирование'],\n",
       "  'requirements': {'vital': ['Ответственность', 'Умение управлять самолётом'],\n",
       "   'extra': ['Владение английским языком']},\n",
       "  'tags': ['пилот', 'самолёт']}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting data from FireBase\n",
    "\n",
    "def all_employees_cvs():\n",
    "    cvs = []\n",
    "    users = db.collection(u'users').stream()\n",
    "    for user in users:\n",
    "        if (u'type' and u'cv') in user.to_dict() and user.get(u'type') == u'employee':\n",
    "            cvs.append(json.loads(user.get(u'cv')))\n",
    "    return cvs\n",
    "\n",
    "\n",
    "def all_hrs_cvs():\n",
    "    cvs = []\n",
    "    users = db.collection(u'users').stream()\n",
    "    for user in users:\n",
    "        if (u'type' and u'cv') in user.to_dict() and user.get(u'type') == u'employer':\n",
    "            cvs.append(json.loads(user.get(u'cv')))\n",
    "    return cvs\n",
    "\n",
    "all_hrs_cvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from JSONs\n",
    "\n",
    "# Test\n",
    "resume = \"test_resume.txt\"                # JSON\n",
    "job_description = \"test_description.txt\"  # JSON\n",
    "\n",
    "resume = open(resume, 'r').read()\n",
    "job_description = open(job_description, 'r').read()\n",
    "\n",
    "resume_data = json.loads(resume)\n",
    "job_data = json.loads(job_description)\n",
    "\n",
    "#for key, value in resume_data.items():\n",
    "#    print(key, value)\n",
    "\n",
    "# !Test\n",
    "\n",
    "#resume_data = all_employees_cvs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [resume, job_description]\n",
    "\n",
    "# Filtering\n",
    "\n",
    "def is_valid(resume_data, job_data):\n",
    "    if resume_data['city'] != job_data['city'] and resume_data['relocate'] == False:\n",
    "        return False\n",
    "    if resume_data['occupation'] != job_data['occupation'] and resume_data['weekend'] != job_data['weekend'] and resume_data['occupation'] != \"-1\":\n",
    "        return False\n",
    "    if job_data['salary'] < resume_data['salary']:\n",
    "        return False\n",
    "    if not list(set(resume_data['tags']) & set(job_data['tags'])):\n",
    "        return False\n",
    "    reqs_length = len(list(set(job_data['requirements']['vital'])))\n",
    "    if len(list(set(resume_data['skills']) & set(job_data['requirements']['vital']))) != reqs_length:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "if not is_valid(resume_data, job_data):\n",
    "    print(\"Candidate is inapropriate for this job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your resume title matches about 77.27272727272727% of the job title.\n",
      "Your resume generally matches about 43.26234269258105% of the job description.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "import Levenshtein\n",
    "resume_title = resume_data['title']\n",
    "job_title = job_data['title']\n",
    "title_match_percentage = (1 - Levenshtein.distance(resume_title, job_title) / len(job_title)) * 100\n",
    "print(\"Your resume title matches about \" + str(title_match_percentage) + \"% of the job title.\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(text)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "match_percentage = cosine_similarity(count_matrix)[0][1] * 100\n",
    "print(\"Your resume generally matches about \" + str(match_percentage) + \"% of the job description.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57.73502691896258, 100.0]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hard & soft skills division\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import operator\n",
    "import math\n",
    "\n",
    "\n",
    "class Extractor():\n",
    "        def __init__(self, resume, job):\n",
    "                self.softskills=self.load_skills('softskills_ru.txt')\n",
    "                self.hardskills=self.load_skills('hardskills_ru.txt')\n",
    "                self.jb_distribution=self.build_ngram_distribution(job)\n",
    "                self.cv_distribution=self.build_ngram_distribution(resume)\n",
    "                self.table=[]\n",
    "\n",
    "        def load_skills(self,filename):\n",
    "                f=open(filename,'r')\n",
    "                skills=[]\n",
    "                for line in f:\n",
    "                        skills.append(self.clean_phrase(line)) \n",
    "                f.close()\n",
    "                return list(set(skills))\n",
    "\n",
    "\n",
    "        def build_ngram_distribution(self,filename):\n",
    "                n_s=[1,2,3]\n",
    "                dist={}\n",
    "                for n in n_s:\n",
    "                        dist.update(self.parse_file(filename,n))\n",
    "                return dist\n",
    "                        \n",
    "\n",
    "        def parse_file(self,filename,n):\n",
    "                f=open(filename,'r')\n",
    "                results={}\n",
    "                for line in f:\n",
    "                        words=self.clean_phrase(line).split(\" \")\n",
    "                        ngrams=self.ngrams(words,n)\n",
    "                        for tup in ngrams:\n",
    "                                phrase=\" \".join(tup)\n",
    "                                if phrase in results.keys():\n",
    "                                        results[phrase]+=1\n",
    "                                else:\n",
    "                                        results[phrase]=1\n",
    "                return results\n",
    "\n",
    "        \n",
    "        def clean_phrase(self,line):\n",
    "                return re.sub(r'[^\\w\\s]','',line.replace('\\n','').replace('\\t','').lower())             \n",
    "\n",
    "\n",
    "        def ngrams(self,input_list, n):\n",
    "                return list(zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "        def measure1(self,v1,v2):\n",
    "                return v1-v2\n",
    "\n",
    "        def measure2(self,v1,v2):\n",
    "                return max(v1-v2,0)\n",
    "\n",
    "        def measure3(self,v1,v2):\n",
    "                sumxx, sumxy, sumyy = 0, 0, 0\n",
    "                for i in range(len(v1)):\n",
    "                        x = v1[i]; y = v2[i]\n",
    "                        sumxx += x*x\n",
    "                        sumyy += y*y\n",
    "                        sumxy += x*y\n",
    "                if math.sqrt(sumxx*sumyy) == 0:\n",
    "                        return 0\n",
    "                return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "\n",
    "        def printMeasures(self, skills_type):\n",
    "                n_rows=len(self.table)\n",
    "                        \n",
    "                for type in [skills_type]:\n",
    "                        v1=[self.table[jb][2] for jb in range(n_rows) if self.table[jb][0]==type]\n",
    "                        v2=[self.table[cv][3] for cv in range(n_rows) if self.table[cv][0]==type]\n",
    "                        return self.measure3(v1,v2) * 100          \n",
    "\n",
    "\n",
    "        def makeTable(self):            \n",
    "                parts_of_speech=['CD','JJ','JJR','JJS','MD','NN','NNS','NNP','NNPS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "                graylist=[\"you\", \"will\"]\n",
    "                tmp_table=[]\n",
    "                \n",
    "                for skill in self.hardskills:\n",
    "                        if skill in self.jb_distribution:\n",
    "                                count_jb=self.jb_distribution[skill]\n",
    "                                if skill in self.cv_distribution:\n",
    "                                        count_cv=self.cv_distribution[skill]\n",
    "                                else:\n",
    "                                        count_cv=0\n",
    "                                m1=self.measure1(count_jb,count_cv)\n",
    "                                m2=self.measure2(count_jb,count_cv)\n",
    "                                tmp_table.append(['hard',skill,count_jb,count_cv,m1,m2])\n",
    "\n",
    "                for skill in self.softskills:\n",
    "                        if skill in self.jb_distribution:\n",
    "                                count_jb=self.jb_distribution[skill]\n",
    "                                if skill in self.cv_distribution:\n",
    "                                        count_cv=self.cv_distribution[skill]\n",
    "                                else:\n",
    "                                        count_cv=0\n",
    "                                m1=self.measure1(count_jb,count_cv)\n",
    "                                m2=self.measure2(count_jb,count_cv)\n",
    "                                tmp_table.append(['soft',skill,count_jb,count_cv,m1,m2])\n",
    "\n",
    "                general_language = sorted(self.jb_distribution.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                for tuple in general_language:\n",
    "                        skill = tuple[0]\n",
    "                        if skill in self.hardskills or skill in self.softskills or skill in graylist:\n",
    "                                continue\n",
    "                        count_jb = tuple[1]\n",
    "                        tokens=nltk.word_tokenize(skill)\n",
    "                        parts=nltk.pos_tag(tokens)\n",
    "                        if all([parts[i][1]in parts_of_speech for i in range(len(parts))]):\n",
    "                                if skill in self.cv_distribution:\n",
    "                                        count_cv=self.cv_distribution[skill]\n",
    "                                else:\n",
    "                                        count_cv=0\n",
    "                                m1=self.measure1(count_jb,count_cv)\n",
    "                                m2=self.measure2(count_jb,count_cv)\n",
    "                                tmp_table.append(['general',skill,count_jb,count_cv,m1,m2])\n",
    "                self.table=tmp_table\n",
    "\n",
    "\n",
    "def calculate_skills():\n",
    "        K=Extractor(\"test_resume.txt\", \"test_description.txt\")\n",
    "        K.makeTable()\n",
    "        hard = K.printMeasures('hard')\n",
    "        soft = K.printMeasures('soft')\n",
    "        return [hard, soft]\n",
    "\n",
    "calculate_skills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your objective matches by 62.66532776720791%.\n",
      "Your education matches by 100%.\n",
      "Your experience matches by 100%.\n",
      "Your skills matched by 78.86751345948129%\n",
      "Your tags matches by 50.0%.\n"
     ]
    }
   ],
   "source": [
    "# Metrics calculation\n",
    "\n",
    "import statistics\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    " \n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1] * 100\n",
    "\n",
    "def calculate_dist(text1, text2):\n",
    "    return (1 - Levenshtein.distance(text1, text2) / max(len(text1), len(text2))) * 100\n",
    "\n",
    "\n",
    "objective_score = calculate_dist(resume_data['objective'], ' '.join(job_data['responsibilities']))\n",
    "if objective_score < match_percentage and objective_score + match_percentage < 100:\n",
    "    objective_score += match_percentage\n",
    "print(\"Your objective matches by \"+str(objective_score)+\"%.\")\n",
    "\n",
    "#TODO: Отдавать предпочтение ВУЗам по специальности тематики вакансий\n",
    "\n",
    "education_score = 100\n",
    "for curr_req in job_data['requirements']['extra']:\n",
    "    for tmp in curr_req:\n",
    "        if \"Образование\" in tmp.split():\n",
    "            education_score = 0\n",
    "            for curr_ed in resume_data['education']:\n",
    "                    if cosine_sim(curr_ed['education_score'], job_data['title']) > 0:\n",
    "                        education_score += match_percentage\n",
    "                    if curr_ed['certified']:\n",
    "                        education += 10\n",
    "education_score = min(100, education_score)\n",
    "print(\"Your education matches by \"+str(education_score)+\"%.\")\n",
    "\n",
    "experience_score = 100\n",
    "experience_count = 0\n",
    "for curr_req in job_data['requirements']['extra']:\n",
    "    for tmp in curr_req:\n",
    "        if \"Опыт\" in tmp.split():\n",
    "            experience_score = 0\n",
    "            for curr_exp in resume_data['experience']:\n",
    "                if (curr_exp['end_date'] > 2016):\n",
    "                    experience_count += (curr_exp['end_date'] - curr_exp['start_date']) + match_percentage\n",
    "                if curr_exp['certified']:\n",
    "                    experience_count += 10\n",
    "experience_count = min(100, experience_count)\n",
    "print(\"Your experience matches by \"+str(experience_score)+\"%.\")\n",
    "\n",
    "skills_score = statistics.mean(calculate_skills())\n",
    "print(\"Your skills matched by \"+str(skills_score)+\"%\")\n",
    "\n",
    "tags_score = len(list(set(resume_data['tags']) & set(job_data['tags']))) / max(len(resume_data['tags']), len(job_data['tags'])) * 100\n",
    "print(\"Your tags matches by \"+str(tags_score)+\"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.7792486259756\n"
     ]
    }
   ],
   "source": [
    "# Linear regression for total matching calculation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "X = df.drop('overall_score', axis=1)\n",
    "y = df['overall_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "#print(regressor.score(X_train, y_train))\n",
    "\n",
    "d = {\"education_score\": [education_score], \"experience_score\": [experience_score], \"skills_score\": [skills_score]}\n",
    "X_input = pd.DataFrame(data=d)\n",
    "overall_score = regressor.predict(X_input)[0]\n",
    "print(min(abs(overall_score), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending back all data to FireBase\n",
    "\n",
    "\n",
    "def send_compute_res(login, d):\n",
    "    db.collection(u'users').document(u'{}'.format(login)).update({u'computation_results': json.dumps(d)})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching between multiple CVs & jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVs parsing from hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF resume to JSON parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "password = b\"\"\n",
    "pagenos = set()\n",
    "maxpages = 0\n",
    "caching = True\n",
    "outtype = \"text\"\n",
    "\n",
    "\n",
    "def _get_content(fname):\n",
    "    rsrcmgr = PDFResourceManager(caching=caching)\n",
    "    laparams = LAParams()\n",
    "    laparams.line_margin = 1.0\n",
    "    laparams.boxes_flow = 1.0\n",
    "    imagewriter = None\n",
    "    with io.BytesIO() as outfp:\n",
    "        device = TextConverter(\n",
    "            rsrcmgr, outfp, laparams=laparams, imagewriter=imagewriter\n",
    "        )\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        with open(fname, \"rb\") as f:\n",
    "            for page in PDFPage.get_pages(\n",
    "                    f,\n",
    "                    pagenos,\n",
    "                    maxpages=maxpages,\n",
    "                    password=password,\n",
    "                    caching=caching,\n",
    "                    check_extractable=True,\n",
    "            ):\n",
    "                interpreter.process_page(page)\n",
    "        return outfp.getvalue().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _parse(content):\n",
    "    \"\"\"Parse the content of the Resume pdf and return the sections with detail\"\"\"\n",
    "    # add NULL to prefix and suffix of the heading\n",
    "    # to easily split the sections\n",
    "    sections = (\n",
    "        section.strip()\n",
    "        for section in re.sub(r\"(\\w+.*\\w+)\\s+_{2,}\", \"\\0\\g<1>\\0\", content).split(\"\\x00\")\n",
    "        if section.strip()\n",
    "    )\n",
    "\n",
    "    # iter_sections = iter(sections)\n",
    "    detail = next(sections)  # this one will be the head contain name, phone and address\n",
    "\n",
    "    # x = [(a,b) for a,b in zip(sections[1::2], sections[2::2])]\n",
    "    x = [(heading, body) for heading, body in zip(sections, sections)]\n",
    "\n",
    "    match = re.search(\n",
    "        r\"(?P<name>\\w+\\s*\\w+)\\s*(?P<phone>\\(\\w+\\)\\s*(\\w+)\\-(\\w+))\\W+(?P<email>.*@.[^ ]*)\\W+(?P<address>.*)\",\n",
    "        detail,\n",
    "    )\n",
    "    details = None\n",
    "    if match:\n",
    "        details = match.groupdict()\n",
    "\n",
    "    details = {k.strip(): v.strip() for k, v in details.items()}\n",
    "\n",
    "    for k, v in x:\n",
    "        details[k] = \"\".join(line.strip() for line in v.strip().split(\"\\n\"))\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "def pdf2json(pdf_name, json_file):\n",
    "    \"\"\"Dump details in JSON from resume pdf\"\"\"\n",
    "    content = _get_content(pdf_name)\n",
    "    data = _parse(content)\n",
    "    open(json_file, \"w\").write(json.dumps(data, indent=4))\n",
    "\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'GoogleCloudServerAccount.json'\n",
    "my_bucket_name = 'hrapp-b56d4.appspot.com'\n",
    "path = \"data/\"\n",
    "\n",
    "\n",
    "def _download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "\n",
    "def _download_user_cv(login, dest):\n",
    "    _download_blob(my_bucket_name, path + login + \"/cv1.pdf\", dest)\n",
    "\n",
    "\n",
    "def user_json_cv(login, json_path):\n",
    "    tmp_path = \"tmp.pdf\"\n",
    "    _download_user_cv(login, tmp_path)\n",
    "    pdf2json(tmp_path, json_path)\n",
    "    os.remove(tmp_path)\n",
    "\n",
    "user_json_cv(\"user2\", 'user2-cv.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
